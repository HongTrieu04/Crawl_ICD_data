import pandas as pd
from neo4j import GraphDatabase
import random
from tqdm import tqdm
import time
import os
import re

# ================= C·∫§U H√åNH =================
URI = "bolt://20.249.211.169:7687"
AUTH = ("neo4j", "neo4j123")
OUTPUT_DIR = "../../data/raw_sentences"
OUTPUT_PREFIX = "raw_sentences"
SENTENCES_PER_FILE = 3000

# M·ª§C TI√äU S·ªê L∆Ø·ª¢NG (90k triplets)
TOTAL_TARGET = 90000
QUOTA = {
    "1-hop": int(TOTAL_TARGET * 0.35), # ~31,500
    "2-hop": int(TOTAL_TARGET * 0.50), # ~45,000
    "3-hop": int(TOTAL_TARGET * 0.15)  # ~13,500
}

MAX_PATHS_PER_DISEASE = {
    "1-hop": 5,   
    "2-hop": 8,
    "3-hop": 3
}

class AdvancedDataGenerator:
    def __init__(self, uri, auth):
        self.driver = GraphDatabase.driver(uri, auth=auth)
        self.collected_data = []
        self.counters = {"1-hop": 0, "2-hop": 0, "3-hop": 0}
        self.file_counter = 1

    def close(self):
        self.driver.close()

    def get_all_diseases(self):
        """L·∫•y danh s√°ch ID t·∫•t c·∫£ c√°c b·ªánh"""
        print("üìã ƒêang l·∫•y danh s√°ch Index c√°c b·ªánh...")
        query = "MATCH (d:Disease) RETURN d.ID as id, d.name as name"
        with self.driver.session() as session:
            result = session.run(query).data()
            random.shuffle(result)
            return result

    # ================= H√ÄM H·ªñ TR·ª¢ X·ª¨ L√ù TEXT =================
    def clean_text(self, text):
        """
        L√†m s·∫°ch v√† tr√≠ch xu·∫•t √Ω ch√≠nh t·ª´ m√¥ t·∫£.
        - Lo·∫°i b·ªè k√Ω t·ª± xu·ªëng d√≤ng.
        - L·∫•y c√¢u ƒë·∫ßu ti√™n tr∆∞·ªõc d·∫•u ch·∫•m.
        """
        if not text or not isinstance(text, str):
            return None
        
        # X√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·∫ßu d√≤ng nh∆∞ ‚Ä¢, -, *
        cleaned = re.sub(r'^[\s‚Ä¢\-\*]+', '', text.strip())
        
        # L·∫•y c√¢u ƒë·∫ßu ti√™n (t√°ch b·ªüi d·∫•u ch·∫•m ho·∫∑c xu·ªëng d√≤ng)
        first_sentence = re.split(r'[.\n]', cleaned)[0]
        
        # N·∫øu c√¢u qu√° ng·∫Øn (d∆∞·ªõi 10 k√Ω t·ª±) ho·∫∑c r·ªóng th√¨ b·ªè qua
        if len(first_sentence) < 10:
            return None
            
        return first_sentence.strip()

    # ================= QUERY BUILDERS (ƒê√É C·∫¨P NH·∫¨T DESCRIPTION) =================
    
    def query_1_hop(self, disease_id, limit):
        """
        1-Hop: L·∫•y th√™m m√¥ t·∫£ c·ªßa B·ªánh, Thu·ªëc, Nh√≥m
        """
        query = """
        MATCH (d:Disease {ID: $id})
        
        OPTIONAL MATCH (d)-[:HAS_SYMPTOM]->(s:Symptom)
        OPTIONAL MATCH (dr:Drug)-[:TREATS]->(d)
        OPTIONAL MATCH (d)-[:BELONGS_TO]->(g:Group)
        
        RETURN 
            d.name as disease,
            d.description as disease_desc,      // <--- Th√™m m√¥ t·∫£ b·ªánh
            s.name as symptom,
            dr.name as drug,
            dr.description as drug_desc,        // <--- Th√™m m√¥ t·∫£ thu·ªëc
            g.name as group_name,
            g.description as group_desc         // <--- Th√™m m√¥ t·∫£ nh√≥m
        LIMIT $limit
        """
        return query

    def query_2_hop(self, disease_id, limit):
        """
        2-Hop: L·∫•y m√¥ t·∫£ thu·ªëc, b·ªánh cha, b·ªánh con
        """
        query = """
        MATCH (d:Disease {ID: $id})
        
        // Path 1: Thu·ªëc -> B·ªánh -> Tri·ªáu ch·ª©ng
        OPTIONAL MATCH (dr:Drug)-[:TREATS]->(d)-[:HAS_SYMPTOM]->(s:Symptom)
        
        // Path 2: B·ªánh con -> B·ªánh cha -> Nh√≥m
        OPTIONAL MATCH (sub:Disease)-[:IS_A]->(d)-[:BELONGS_TO]->(g:Group)
        
        WITH d, dr, s, sub, g
        WHERE dr IS NOT NULL OR sub IS NOT NULL 

        RETURN 
            d.name as disease,
            d.description as disease_desc,      // <---
            dr.name as drug,
            dr.description as drug_desc,        // <---
            s.name as symptom,
            sub.name as sub_disease,
            sub.description as sub_desc,        // <---
            g.name as group_name
        LIMIT $limit
        """
        return query

    def query_3_hop(self, disease_id, limit):
        """
        3-Hop: L·∫•y m√¥ t·∫£ Nh√≥m v√† Ch∆∞∆°ng
        """
        query = """
        MATCH (d:Disease {ID: $id})
        MATCH (dr:Drug)-[:TREATS]->(d)-[:BELONGS_TO]->(g:Group)-[:BELONGS_TO]->(c:Chapter)
        
        RETURN 
            d.name as disease,
            dr.name as drug,
            g.name as group_name,
            g.description as group_desc,        // <---
            c.name as chapter_name,
            c.description as chapter_desc       // <---
        LIMIT $limit
        """
        return query

    # ================= TEMPLATE APPLIER (ƒê√É N√ÇNG C·∫§P) =================
    
    def process_result_to_text(self, record, hop_type):
        """Chuy·ªÉn k·∫øt qu·∫£ DB th√†nh c√¢u vƒÉn (K·∫øt h·ª£p m√¥ t·∫£)"""
        items = []
        
        # L·∫•y d·ªØ li·ªáu c∆° b·∫£n
        d = record.get('disease')
        d_desc = self.clean_text(record.get('disease_desc'))
        
        if hop_type == "1-hop":
            # --- X·ª≠ l√Ω Thu·ªëc ---
            if record.get('drug'):
                dr = record['drug']
                dr_desc = self.clean_text(record.get('drug_desc'))
                
                # Template c∆° b·∫£n
                base = f"{dr} l√† thu·ªëc ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho {d}."
                items.append(base)
                
                # Template n√¢ng cao (n·∫øu c√≥ m√¥ t·∫£ thu·ªëc)
                if dr_desc:
                    items.append(f"Thu·ªëc {dr} ({dr_desc.lower()}) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëi·ªÅu tr·ªã {d}.")
            
            # --- X·ª≠ l√Ω Nh√≥m ---
            if record.get('group_name'):
                grp = record['group_name']
                grp_desc = self.clean_text(record.get('group_desc'))
                
                items.append(f"{d} ƒë∆∞·ª£c ph√¢n lo·∫°i thu·ªôc nh√≥m {grp}.")
                if grp_desc:
                    items.append(f"Nh√≥m b·ªánh {grp}, bao g·ªìm {grp_desc.lower()}, ch·ª©a c√°c b·ªánh l√Ω nh∆∞ {d}.")

            # --- X·ª≠ l√Ω M√¥ t·∫£ B·ªánh (R·∫•t quan tr·ªçng) ---
            if d_desc:
                items.append(f"V·ªÅ m·∫∑t l√¢m s√†ng, {d} l√† t√¨nh tr·∫°ng {d_desc.lower()}.")
                items.append(f"ƒê·ªãnh nghƒ©a: {d_desc}.")

            # --- X·ª≠ l√Ω Tri·ªáu ch·ª©ng ---
            if record.get('symptom'):
                items.append(f"M·ªôt trong nh·ªØng d·∫•u hi·ªáu c·ªßa {d} l√† {record['symptom']}.")

        elif hop_type == "2-hop":
            # --- Path: Thu·ªëc -> B·ªánh -> Tri·ªáu ch·ª©ng ---
            if record.get('drug') and record.get('symptom'):
                dr = record['drug']
                s = record['symptom']
                dr_desc = self.clean_text(record.get('drug_desc'))
                
                # Template ng·ªØ c·∫£nh ƒëi·ªÅu tr·ªã tri·ªáu ch·ª©ng
                if d_desc:
                    items.append(f"ƒê·ªëi v·ªõi {d} ({d_desc.lower()}), thu·ªëc {dr} c√≥ th·ªÉ ƒë∆∞·ª£c d√πng khi b·ªánh nh√¢n c√≥ bi·ªÉu hi·ªán {s}.")
                else:
                    items.append(f"B·ªánh nh√¢n {d} c√≥ tri·ªáu ch·ª©ng {s} th∆∞·ªùng ƒë∆∞·ª£c ƒëi·ªÅu tr·ªã b·∫±ng {dr}.")
            
            # --- Path: B·ªánh con -> B·ªánh cha -> Nh√≥m ---
            if record.get('sub_disease') and record.get('group_name'):
                sub = record['sub_disease']
                sub_desc = self.clean_text(record.get('sub_desc'))
                grp = record['group_name']
                
                base = f"{sub} l√† m·ªôt bi·∫øn th·ªÉ c·ªßa {d}, n·∫±m trong nh√≥m {grp}."
                items.append(base)
                
                if sub_desc:
                    items.append(f"{sub} ({sub_desc.lower()}) ƒë∆∞·ª£c x·∫øp v√†o nh√≥m {grp} c√πng v·ªõi {d}.")

        elif hop_type == "3-hop":
            # --- Path: Drug -> Disease -> Group -> Chapter ---
            if record.get('drug') and record.get('group_name') and record.get('chapter_name'):
                dr = record['drug']
                grp = record['group_name']
                chap = record['chapter_name']
                chap_desc = self.clean_text(record.get('chapter_desc'))
                
                items.append(f"Thu·ªëc {dr} ƒëi·ªÅu tr·ªã {d} (nh√≥m {grp}), thu·ªôc ch∆∞∆°ng {chap}.")
                
                if chap_desc:
                    items.append(f"Trong ch∆∞∆°ng {chap} ({chap_desc.lower()}), {d} thu·ªôc nh√≥m {grp} v√† c√≥ th·ªÉ ƒëi·ªÅu tr·ªã b·∫±ng {dr}.")

        return items

    # ================= FILE SAVING =================
    
    def save_batch(self):
        if not self.collected_data: return
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        filename = f"{OUTPUT_PREFIX}_part{self.file_counter:03d}.csv"
        filepath = os.path.join(OUTPUT_DIR, filename)
        
        df = pd.DataFrame(self.collected_data)
        df = df.sample(frac=1).reset_index(drop=True)
        df['label'] = True
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"üíæ ƒê√£ l∆∞u {len(self.collected_data)} c√¢u v√†o: {filename}")
        
        self.collected_data = []
        self.file_counter += 1

    # ================= MAIN GENERATOR =================

    def generate(self):
        all_diseases = self.get_all_diseases()
        total_diseases = len(all_diseases)
        print(f"‚úÖ T√¨m th·∫•y {total_diseases} b·ªánh. B·∫Øt ƒë·∫ßu sampling...")

        with self.driver.session() as session:
            pbar = tqdm(total=TOTAL_TARGET)
            
            for idx, disease_info in enumerate(all_diseases):
                d_id = disease_info['id']
                if all(self.counters[k] >= QUOTA[k] for k in QUOTA):
                    print("\nüéâ ƒê√£ ƒë·∫°t ƒë·ªß ch·ªâ ti√™u s·ªë l∆∞·ª£ng!")
                    break

                # 1-HOP
                if self.counters["1-hop"] < QUOTA["1-hop"]:
                    res = session.run(self.query_1_hop(d_id, MAX_PATHS_PER_DISEASE["1-hop"]), id=d_id, limit=MAX_PATHS_PER_DISEASE["1-hop"]).data()
                    for r in res:
                        sentences = self.process_result_to_text(r, "1-hop")
                        for s in sentences:
                            self.collected_data.append({"text": s, "hop": "1-hop", "source_id": d_id})
                            self.counters["1-hop"] += 1
                            pbar.update(1)
                            if len(self.collected_data) >= SENTENCES_PER_FILE: self.save_batch()

                # 2-HOP
                if self.counters["2-hop"] < QUOTA["2-hop"]:
                    res = session.run(self.query_2_hop(d_id, MAX_PATHS_PER_DISEASE["2-hop"]), id=d_id, limit=MAX_PATHS_PER_DISEASE["2-hop"]).data()
                    for r in res:
                        sentences = self.process_result_to_text(r, "2-hop")
                        for s in sentences:
                            self.collected_data.append({"text": s, "hop": "2-hop", "source_id": d_id})
                            self.counters["2-hop"] += 1
                            pbar.update(1)
                            if len(self.collected_data) >= SENTENCES_PER_FILE: self.save_batch()

                # 3-HOP
                if self.counters["3-hop"] < QUOTA["3-hop"]:
                    res = session.run(self.query_3_hop(d_id, MAX_PATHS_PER_DISEASE["3-hop"]), id=d_id, limit=MAX_PATHS_PER_DISEASE["3-hop"]).data()
                    for r in res:
                        sentences = self.process_result_to_text(r, "3-hop")
                        for s in sentences:
                            self.collected_data.append({"text": s, "hop": "3-hop", "source_id": d_id})
                            self.counters["3-hop"] += 1
                            pbar.update(1)
                            if len(self.collected_data) >= SENTENCES_PER_FILE: self.save_batch()

            pbar.close()

        if self.collected_data:
            self.save_batch()

        print("\nüìä Th·ªëng k√™ k·∫øt qu·∫£:")
        print(f"   - 1-hop: {self.counters['1-hop']} c√¢u")
        print(f"   - 2-hop: {self.counters['2-hop']} c√¢u")
        print(f"   - 3-hop: {self.counters['3-hop']} c√¢u")
        print(f"   - T·ªïng s·ªë file: {self.file_counter - 1} files")
        print(f"‚úÖ C√°c file ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i th∆∞ m·ª•c: {OUTPUT_DIR}")

if __name__ == "__main__":
    generator = AdvancedDataGenerator(URI, AUTH)
    try:
        generator.generate()
    finally:
        generator.close()


# import pandas as pd
# from neo4j import GraphDatabase
# import random
# from tqdm import tqdm
# import time
# import os

# # C·∫§U H√åNH NEO4J - THAY ƒê·ªîI TH√îNG TIN C·ª¶A B·∫†N ·ªû ƒê√ÇY
# URI = "bolt://20.249.211.169:7687" 
# AUTH = ("neo4j", "neo4j123")     

# # C·∫§U H√åNH OUTPUT - L∆∞u v√†o Google Drive
# OUTPUT_DIR = "Data/raw_sentences"
# OUTPUT_PREFIX = "raw_sentences"
# SENTENCES_PER_FILE = 3000

# # M·ª§C TI√äU S·ªê L∆Ø·ª¢NG (90k c√¢u)
# TOTAL_TARGET = 90000
# QUOTA = {
#     "1-hop": int(TOTAL_TARGET * 0.35), # ~31,500
#     "2-hop": int(TOTAL_TARGET * 0.50), # ~45,000
#     "3-hop": int(TOTAL_TARGET * 0.15)  # ~13,500
# }

# # GI·ªöI H·∫†N ƒê·ªÇ ƒê·∫¢M B·∫¢O ƒêA D·∫†NG
# MAX_PATHS_PER_DISEASE = {
#     "1-hop": 5,   
#     "2-hop": 8,
#     "3-hop": 3
# }

# print("‚öôÔ∏è C·∫•u h√¨nh ho√†n t·∫•t!")
# print(f"üìç Neo4j URI: {URI}")
# print(f"üíæ Output folder: {OUTPUT_DIR}")
# class AdvancedDataGenerator:
#     def __init__(self, uri, auth):
#         self.driver = GraphDatabase.driver(uri, auth=auth)
#         self.collected_data = []
#         self.counters = {"1-hop": 0, "2-hop": 0, "3-hop": 0}
#         self.file_counter = 1

#     def close(self):
#         self.driver.close()

#     def get_all_diseases(self):
#         """L·∫•y danh s√°ch ID t·∫•t c·∫£ c√°c b·ªánh ƒë·ªÉ sample ng·∫´u nhi√™n"""
#         print("üìã ƒêang l·∫•y danh s√°ch Index c√°c b·ªánh...")
#         query = "MATCH (d:Disease) RETURN d.ID as id, d.name as name"
#         with self.driver.session() as session:
#             result = session.run(query).data()
#             random.shuffle(result)
#             return result

#     # ================= QUERY BUILDERS =================
    
#     def query_1_hop(self, disease_id, limit):
#         """1-Hop: Quan h·ªá tr·ª±c ti·∫øp."""
#         query = """
#         MATCH (d:Disease {ID: $id})
#         OPTIONAL MATCH (d)-[:HAS_SYMPTOM]->(s:Symptom)
#         OPTIONAL MATCH (dr:Drug)-[:TREATS]->(d)
#         OPTIONAL MATCH (d)-[:BELONGS_TO]->(g:Group)
        
#         RETURN 
#             d.name as disease,
#             s.name as symptom,
#             dr.name as drug,
#             g.name as group_name,
#             d.description as description
#         LIMIT $limit
#         """
#         return query

#     def query_2_hop(self, disease_id, limit):
#         """2-Hop: Ng·ªØ c·∫£nh phong ph√∫."""
#         query = """
#         MATCH (d:Disease {ID: $id})
        
#         OPTIONAL MATCH (dr:Drug)-[:TREATS]->(d)-[:HAS_SYMPTOM]->(s:Symptom)
#         OPTIONAL MATCH (sub:Disease)-[:IS_A]->(d)-[:BELONGS_TO]->(g:Group)
        
#         WITH d, dr, s, sub, g
#         WHERE dr IS NOT NULL OR sub IS NOT NULL

#         RETURN 
#             d.name as disease,
#             dr.name as drug,
#             s.name as symptom,
#             sub.name as sub_disease,
#             g.name as group_name
#         LIMIT $limit
#         """
#         return query

#     def query_3_hop(self, disease_id, limit):
#         """3-Hop: Ng·ªØ c·∫£nh s√¢u."""
#         query = """
#         MATCH (d:Disease {ID: $id})
#         MATCH (dr:Drug)-[:TREATS]->(d)-[:BELONGS_TO]->(g:Group)-[:BELONGS_TO]->(c:Chapter)
        
#         RETURN 
#             d.name as disease,
#             dr.name as drug,
#             g.name as group_name,
#             c.name as chapter_name
#         LIMIT $limit
#         """
#         return query

#     # ================= TEMPLATE APPLIER =================
    
#     def process_result_to_text(self, record, hop_type):
#         """Chuy·ªÉn k·∫øt qu·∫£ Raw DB th√†nh c√¢u th√¥"""
#         items = []
#         d = record.get('disease')
        
#         if hop_type == "1-hop":
#             if record.get('symptom'):
#                 items.append(f"B·ªánh {d} c√≥ bi·ªÉu hi·ªán l√¢m s√†ng l√† {record['symptom']}.")
#             if record.get('drug'):
#                 items.append(f"{record['drug']} l√† thu·ªëc ƒë∆∞·ª£c d√πng cho {d}.")
#             if record.get('group_name'):
#                 items.append(f"{d} thu·ªôc nh√≥m b·ªánh {record['group_name']}.")
#             if record.get('description'):
#                 desc = record['description'].split('.')[0]
#                 if len(desc) > 20:
#                     items.append(f"V·ªÅ {d}: {desc}.")

#         elif hop_type == "2-hop":
#             if record.get('drug') and record.get('symptom'):
#                 items.append(f"B·ªánh nh√¢n {d} c√≥ tri·ªáu ch·ª©ng {record['symptom']} c√≥ th·ªÉ ƒë∆∞·ª£c ƒëi·ªÅu tr·ªã b·∫±ng {record['drug']}.")
#             if record.get('sub_disease') and record.get('group_name'):
#                 items.append(f"{record['sub_disease']} l√† m·ªôt bi·∫øn th·ªÉ c·ªßa {d}, thu·ªôc nh√≥m {record['group_name']}.")

#         elif hop_type == "3-hop":
#             if record.get('drug') and record.get('group_name') and record.get('chapter_name'):
#                  items.append(f"Thu·ªëc {record['drug']} ƒëi·ªÅu tr·ªã {d} (nh√≥m {record['group_name']}), thu·ªôc ch∆∞∆°ng {record['chapter_name']}.")

#         return items

#     # ================= FILE SAVING =================
    
#     def save_batch(self):
#         """L∆∞u batch hi·ªán t·∫°i ra file v√† reset collected_data"""
#         if not self.collected_data:
#             return
        
#         # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
#         os.makedirs(OUTPUT_DIR, exist_ok=True)
        
#         # T·∫°o t√™n file v·ªõi s·ªë th·ª© t·ª±
#         filename = f"{OUTPUT_PREFIX}_part{self.file_counter:03d}.csv"
#         filepath = os.path.join(OUTPUT_DIR, filename)
        
#         # Shuffle tr∆∞·ªõc khi l∆∞u
#         df = pd.DataFrame(self.collected_data)
#         df = df.sample(frac=1).reset_index(drop=True)
#         df['label'] = True
        
#         # L∆∞u file
#         df.to_csv(filepath, index=False, encoding='utf-8-sig')
#         print(f"üíæ ƒê√£ l∆∞u {len(self.collected_data)} c√¢u v√†o: {filename}")
        
#         # Reset v√† tƒÉng counter
#         self.collected_data = []
#         self.file_counter += 1

#     # ================= MAIN GENERATOR =================

#     def generate(self):
#         all_diseases = self.get_all_diseases()
#         total_diseases = len(all_diseases)
#         print(f"‚úÖ T√¨m th·∫•y {total_diseases} b·ªánh. B·∫Øt ƒë·∫ßu sampling...")

#         with self.driver.session() as session:
#             pbar = tqdm(total=TOTAL_TARGET)
            
#             for idx, disease_info in enumerate(all_diseases):
#                 d_id = disease_info['id']
                
#                 # Ki·ªÉm tra ƒë√£ ƒë·ªß quota ch∆∞a
#                 if all(self.counters[k] >= QUOTA[k] for k in QUOTA):
#                     print("\nüéâ ƒê√£ ƒë·∫°t ƒë·ªß ch·ªâ ti√™u s·ªë l∆∞·ª£ng!")
#                     break

#                 # --- 1-HOP ---
#                 if self.counters["1-hop"] < QUOTA["1-hop"]:
#                     res = session.run(self.query_1_hop(d_id, MAX_PATHS_PER_DISEASE["1-hop"]), 
#                                      id=d_id, limit=MAX_PATHS_PER_DISEASE["1-hop"]).data()
#                     for r in res:
#                         sentences = self.process_result_to_text(r, "1-hop")
#                         for s in sentences:
#                             self.collected_data.append({"text": s, "hop": "1-hop", "source_id": d_id})
#                             self.counters["1-hop"] += 1
#                             pbar.update(1)
                            
#                             if len(self.collected_data) >= SENTENCES_PER_FILE:
#                                 self.save_batch()

#                 # --- 2-HOP ---
#                 if self.counters["2-hop"] < QUOTA["2-hop"]:
#                     res = session.run(self.query_2_hop(d_id, MAX_PATHS_PER_DISEASE["2-hop"]), 
#                                      id=d_id, limit=MAX_PATHS_PER_DISEASE["2-hop"]).data()
#                     for r in res:
#                         sentences = self.process_result_to_text(r, "2-hop")
#                         for s in sentences:
#                             self.collected_data.append({"text": s, "hop": "2-hop", "source_id": d_id})
#                             self.counters["2-hop"] += 1
#                             pbar.update(1)
                            
#                             if len(self.collected_data) >= SENTENCES_PER_FILE:
#                                 self.save_batch()

#                 # --- 3-HOP ---
#                 if self.counters["3-hop"] < QUOTA["3-hop"]:
#                     res = session.run(self.query_3_hop(d_id, MAX_PATHS_PER_DISEASE["3-hop"]), 
#                                      id=d_id, limit=MAX_PATHS_PER_DISEASE["3-hop"]).data()
#                     for r in res:
#                         sentences = self.process_result_to_text(r, "3-hop")
#                         for s in sentences:
#                             self.collected_data.append({"text": s, "hop": "3-hop", "source_id": d_id})
#                             self.counters["3-hop"] += 1
#                             pbar.update(1)
                            
#                             if len(self.collected_data) >= SENTENCES_PER_FILE:
#                                 self.save_batch()

#             pbar.close()

#         # L∆∞u ph·∫ßn c√≤n l·∫°i
#         if self.collected_data:
#             self.save_batch()

#         # Th·ªëng k√™
#         print("\nüìä Th·ªëng k√™ k·∫øt qu·∫£:")
#         print(f"   - 1-hop: {self.counters['1-hop']} c√¢u")
#         print(f"   - 2-hop: {self.counters['2-hop']} c√¢u")
#         print(f"   - 3-hop: {self.counters['3-hop']} c√¢u")
#         print(f"   - T·ªïng s·ªë file: {self.file_counter - 1} files")
#         print(f"‚úÖ C√°c file ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}")
# # ============= B∆Ø·ªöC 5: Ki·ªÉm tra k·∫øt n·ªëi Neo4j =============
# print("\nüîå ƒêang ki·ªÉm tra k·∫øt n·ªëi Neo4j...")
# try:
#     test_driver = GraphDatabase.driver(URI, auth=AUTH)
#     with test_driver.session() as session:
#         result = session.run("RETURN 1 as test")
#         result.single()
#     test_driver.close()
#     print("‚úÖ K·∫øt n·ªëi Neo4j th√†nh c√¥ng!")
# except Exception as e:
#     print(f"‚ùå L·ªói k·∫øt n·ªëi Neo4j: {e}")
#     print("‚ö†Ô∏è Vui l√≤ng ki·ªÉm tra l·∫°i URI v√† AUTH")

# # ============= B∆Ø·ªöC 6: Ch·∫°y Generator =============
# print("\nüöÄ B·∫Øt ƒë·∫ßu generate data...")
# print("=" * 60)

# generator = AdvancedDataGenerator(URI, AUTH)
# try:
#     generator.generate()
# except Exception as e:
#     print(f"‚ùå L·ªói khi ch·∫°y: {e}")
#     import traceback
#     traceback.print_exc()
# finally:
#     generator.close()
#     print("\n‚úÖ Ho√†n t·∫•t! Ki·ªÉm tra Google Drive c·ªßa b·∫°n.")

# # ============= B∆Ø·ªöC 7: Ki·ªÉm tra k·∫øt qu·∫£ =============
# print("\nüìÇ Danh s√°ch file ƒë√£ t·∫°o:")
# if os.path.exists(OUTPUT_DIR):
#     files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.endswith('.csv')])
#     for i, f in enumerate(files, 1):
#         filepath = os.path.join(OUTPUT_DIR, f)
#         size_mb = os.path.getsize(filepath) / (1024 * 1024)
#         print(f"   {i}. {f} ({size_mb:.2f} MB)")
# else:
#     print("   ‚ö†Ô∏è Ch∆∞a c√≥ file n√†o ƒë∆∞·ª£c t·∫°o")