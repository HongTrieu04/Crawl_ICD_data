import torch
import pandas as pd
import torch.nn.functional as F
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ================= C·∫§U H√åNH =================
# ƒê∆∞·ªùng d·∫´n folder ch·ª©a Adapter (k·∫øt qu·∫£ sau khi train xong)
ADAPTER_PATH = "../../models/qwen3_slm_batch6/checkpoint-3800" 
# T√™n ho·∫∑c ƒë∆∞·ªùng d·∫´n model g·ªëc (B·∫Øt bu·ªôc ph·∫£i c√≥ ƒë·ªÉ l√†m n·ªÅn)
BASE_MODEL_PATH = "../../models/qwen3-0.6b/models--Qwen--Qwen3-0.6B" 

INPUT_FILE = "../../data/data_test_normalize.csv"
OUTPUT_FILE = "../../data/Result_Vector_Injection.xlsx"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= CORE LOGIC =================
class VectorInference:
    def __init__(self):
        print(f"üöÄ ƒêang kh·ªüi t·∫°o tr√™n thi·∫øt b·ªã: {DEVICE}")
        
        # 1. Load Tokenizer (∆Øu ti√™n load t·ª´ Adapter n·∫øu c√≥, kh√¥ng th√¨ l·∫•y t·ª´ Base)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)
        except:
            self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
            
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # 2. Load Base Model (Model n·ªÅn)
        print(f"üì¶ Loading Base Model: {BASE_MODEL_PATH}...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            device_map=DEVICE,
            torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
            trust_remote_code=True
        )
        
        # Resize token embeddings n·∫øu l√∫c train b·∫°n c√≥ resize (Quan tr·ªçng!)
        self.base_model.resize_token_embeddings(len(self.tokenizer))

        # 3. Load & G·∫Øn Adapter (LoRA)
        print(f"üîó Loading Adapter t·ª´: {ADAPTER_PATH}...")
        self.model = PeftModel.from_pretrained(self.base_model, ADAPTER_PATH)
        self.model.eval()
        
        # 4. X√°c ƒë·ªãnh Token ID c·ªßa nh√£n "ƒê√∫ng"/"Sai"
        # Tokenizer ƒë√¥i khi th√™m kho·∫£ng tr·∫Øng (vd: "_ƒê√∫ng"), n√™n ta encode v√† l·∫•y token cu·ªëi c√πng cho ch·∫Øc
        self.true_token_id = self.get_single_token_id("ƒê√∫ng")
        self.false_token_id = self.get_single_token_id("Sai")
        
        print(f"‚ÑπÔ∏è Token Map: 'ƒê√∫ng' -> ID {self.true_token_id} | 'Sai' -> ID {self.false_token_id}")

    def get_single_token_id(self, word):
        """H√†m helper ƒë·ªÉ l·∫•y ID c·ªßa 1 t·ª´ ƒë∆°n"""
        ids = self.tokenizer.encode(word, add_special_tokens=False)
        return ids[-1] if ids else -1

    def predict_from_vector(self, context, statement):
        """
        Quy tr√¨nh Vector Injection:
        Text -> Token IDs -> Embeddings Layer (Base Model) -> Vectors -> Transformer Layers -> Logits
        """
        
        # Format Prompt: PH·∫¢I GI·ªêNG H·ªÜT L√öC TRAIN ƒë·ªÉ ƒë·∫°t hi·ªáu qu·∫£ cao nh·∫•t
        display_context = context if pd.notna(context) and str(context).strip() else statement
        display_statement = statement if pd.notna(statement) and str(statement).strip() else display_context
        
        prompt_text = (
            f"Ng·ªØ c·∫£nh: {display_context}\n"
            f"M·ªánh ƒë·ªÅ: {display_statement}\n"
            "H√£y ph√¢n lo·∫°i m·ªánh ƒë·ªÅ tr√™n l√† 'ƒê√∫ng' ho·∫∑c 'Sai'. "
            "Ch·ªâ tr·∫£ l·ªùi ƒë√∫ng m·ªôt t·ª´: ƒê√∫ng ho·∫∑c Sai.\n"
            "C√¢u tr·∫£ l·ªùi:" # Th√™m g·ª£i √Ω ƒë·ªÉ model ƒëi·ªÅn ti·∫øp
        )
        
        # B1: Tokenize
        inputs = self.tokenizer(prompt_text, return_tensors="pt").to(self.model.device)
        input_ids = inputs.input_ids
        
        # B2: VECTOR EMBEDDING (ƒê√¢y l√† b∆∞·ªõc b·∫°n y√™u c·∫ßu)
        # L·∫•y l·ªõp Embedding ƒë·∫ßu ti√™n c·ªßa model g·ªëc
        # Qwen/Llama th∆∞·ªùng c√≥ thu·ªôc t√≠nh .model.embed_tokens ho·∫∑c .get_input_embeddings()
        with torch.no_grad():
            # Bi·∫øn ƒë·ªïi Token IDs (s·ªë nguy√™n) th√†nh Vectors (s·ªë th·ª±c float16)
            # input_vectors shape: [1, seq_len, hidden_size]
            input_vectors = self.model.get_input_embeddings()(input_ids)
        
        # B3: FORWARD PASS B·∫∞NG VECTOR
        with torch.no_grad():
            # Thay v√¨ truy·ªÅn input_ids, ta truy·ªÅn inputs_embeds
            outputs = self.model(
                inputs_embeds=input_vectors,
                # V·∫´n c·∫ßn attention_mask ƒë·ªÉ model bi·∫øt ƒë√¢u l√† padding
                attention_mask=inputs.attention_mask 
            )
            
            # L·∫•y Logits c·ªßa token cu·ªëi c√πng (token d·ª± ƒëo√°n ti·∫øp theo)
            next_token_logits = outputs.logits[0, -1, :]
            
            # So s√°nh ƒëi·ªÉm s·ªë (Logits) gi·ªØa token "ƒê√∫ng" v√† "Sai"
            score_true = next_token_logits[self.true_token_id].item()
            score_false = next_token_logits[self.false_token_id].item()
            
            # Chuy·ªÉn sang x√°c su·∫•t (Softmax c·ª•c b·ªô gi·ªØa 2 token n√†y)
            probs = F.softmax(torch.tensor([score_true, score_false], dtype=torch.float32), dim=0)
            prob_true = probs[0].item()
            prob_false = probs[1].item()

        # K·∫øt lu·∫≠n
        if prob_true > prob_false:
            return "ƒê√∫ng", prob_true
        else:
            return "Sai", prob_false

# ================= MAIN =================
def run():
    # Kh·ªüi t·∫°o Engine
    try:
        engine = VectorInference()
    except Exception as e:
        print(f"‚ùå L·ªói kh·ªüi t·∫°o model: {e}")
        return

    # ƒê·ªçc d·ªØ li·ªáu
    print(f"üìÇ ƒêang ƒë·ªçc file {INPUT_FILE}...")
    try:
        if INPUT_FILE.endswith('.csv'):
            df = pd.read_csv(INPUT_FILE)
        else:
            df = pd.read_excel(INPUT_FILE)
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file input!")
        return

    # T·ª± ƒë·ªông t√¨m t√™n c·ªôt
    col_stmt = next((c for c in df.columns if "statement" in c.lower() or "m·ªánh ƒë·ªÅ" in c.lower()), "statement")
    col_ctx = next((c for c in df.columns if "context" in c.lower() or "ng·ªØ c·∫£nh" in c.lower()), "context")
    
    results = []
    print(f"‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ch·∫°y Inference tr√™n {len(df)} d√≤ng...")
    
    # D√πng tqdm ƒë·ªÉ hi·ªán thanh ti·∫øn tr√¨nh
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        stmt = row.get(col_stmt, "")
        ctx = row.get(col_ctx, "")
        
        # G·ªçi h√†m x·ª≠ l√Ω vector
        decision, confidence = engine.predict_from_vector(ctx, stmt)
        
        results.append({
            "Context": ctx,
            "Statement": stmt,
            "Prediction": decision,
            "Confidence": round(confidence, 4)
        })
        
    # L∆∞u k·∫øt qu·∫£
    out_df = pd.DataFrame(results)
    out_df.to_excel(OUTPUT_FILE, index=False)
    print(f"\n‚úÖ ƒê√£ xong! K·∫øt qu·∫£ l∆∞u t·∫°i: {OUTPUT_FILE}")
    print(out_df[['Statement', 'Prediction', 'Confidence']].head())

if __name__ == "__main__":
    run()