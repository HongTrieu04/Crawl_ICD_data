# -*- coding: utf-8 -*-
"""RAG_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gb9b3wFZrp2Al4JQh9_KzRZxIBQ9RERX
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q torch transformers peft accelerate pandas faiss-cpu sentence-transformers langchain langchain-community

"""# **RAG trÃªn toÃ n bá»™ táº­p dá»¯u liá»‡u bao gá»“m cáº£ file dá»¯ liá»‡u Ä‘Ãºng vÃ  sai**"""

import pandas as pd
import json
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
# --- Sá»¬A Lá»–I IMPORT Táº I ÄÃ‚Y ---
from langchain_core.documents import Document

# ================= 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN =================
# ÄÆ°á»ng dáº«n Ä‘áº¿n file chá»©a toÃ n dá»¯ liá»‡u ÄÃšNG
TRAIN_TRUE_PATH = "/content/drive/MyDrive/KPDLHLV/Data/checkpoint_results.jsonl"

# ÄÆ°á»ng dáº«n Ä‘áº¿n file chá»©a toÃ n dá»¯ liá»‡u SAI
TRAIN_FALSE_PATH = "/content/drive/MyDrive/KPDLHLV/Data/checkpoint_results_sai.jsonl"

# Model Embedding
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ================= 2. HÃ€M LOAD Dá»® LIá»†U =================
def load_data_to_docs(file_paths):
    docs = []
    total_loaded = 0

    for path in file_paths:
        if not os.path.exists(path):
            print(f"âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y file {path}, bá» qua.")
            continue

        print(f"ğŸ“‚ Äang Ä‘á»c file: {path}...")
        count = 0
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    line = line.strip()
                    if not line: continue

                    item = json.loads(line)

                    context = item.get('context', '')
                    statement = item.get('statement', '')
                    answer = item.get('answer', '')

                    if not answer:
                        if "sai" in path.lower(): answer = "Sai"
                        else: answer = "ÄÃºng"

                    # Ná»™i dung Ä‘á»ƒ Embedding (TÃ¬m kiáº¿m)
                    content_for_search = f"Ngá»¯ cáº£nh: {context}\nMá»‡nh Ä‘á»: {statement}"

                    # Metadata (ThÃ´ng tin gá»‘c)
                    metadata = {
                        "original_context": context,
                        "original_statement": statement,
                        "answer": answer
                    }

                    docs.append(Document(page_content=content_for_search, metadata=metadata))
                    count += 1
                except Exception as e:
                    continue

        print(f"   -> ÄÃ£ láº¥y {count} máº«u tá»« file nÃ y.")
        total_loaded += count

    print(f"âœ… Tá»”NG Cá»˜NG: ÄÃ£ náº¡p {total_loaded} máº«u dá»¯ liá»‡u vÃ o bá»™ nhá»›.")
    return docs

# ================= 3. THá»°C THI =================

# Danh sÃ¡ch cÃ¡c file cáº§n load
list_files = [TRAIN_TRUE_PATH, TRAIN_FALSE_PATH]

# Load dá»¯ liá»‡u
documents = load_data_to_docs(list_files)

if len(documents) > 0:
    print("\nğŸš€ Äang táº¡o Embedding vÃ  Index vÃ o FAISS...")
    print("â³ QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...")

    # Load model embedding
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'} # Cháº¡y trÃªn GPU
    )

    # Táº¡o Vector Database
    db = FAISS.from_documents(documents, embedding_model)

    print("âœ… ÄÃ£ xÃ¢y xong RAG Database thÃ nh cÃ´ng!")
else:
    print("âŒ Lá»—i: KhÃ´ng load Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o.")

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

# ================= Cáº¤U HÃŒNH MODEL FINETUNE =================
ADAPTER_PATH = "/content/drive/MyDrive/KPDLHLV/qwen3_slm_batch6/checkpoint-3800"
BASE_MODEL_ID = "Qwen/Qwen3-0.6B" # (Thá»±c ra lÃ  Qwen2.5-0.5B hoáº·c Qwen2-0.5B, báº¡n check láº¡i Ä‘Ãºng tÃªn nhÃ©)

TEST_FILE = "/content/drive/MyDrive/KPDLHLV/Data/data_test_normalize.csv"
OUTPUT_FILE = "/content/drive/MyDrive/KPDLHLV/Data/Result_RAG_Finetune.xlsx"
DEVICE = "cuda"

# ================= LOAD QWEN MODEL =================
print("ğŸ“¦ Äang load Qwen Finetuned Model...")
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)
if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    device_map=DEVICE,
    torch_dtype=torch.float16,
    trust_remote_code=True
)
base_model.resize_token_embeddings(len(tokenizer))

model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.eval()
print("âœ… Qwen Ä‘Ã£ sáºµn sÃ ng!")

# ================= HÃ€M RAG QUERY =================
def generate_rag_prompt(query_context, query_statement, k=3):
    """
    HÃ m nÃ y tÃ¬m 3 vÃ­ dá»¥ tÆ°Æ¡ng tá»± nháº¥t trong dá»¯ liá»‡u train
    Ä‘á»ƒ lÃ m 'gá»£i Ã½' cho model.
    """
    query_text = f"Ngá»¯ cáº£nh: {query_context}\nMá»‡nh Ä‘á»: {query_statement}"

    # 1. TÃ¬m kiáº¿m trong FAISS
    # k=3: Láº¥y 3 vÃ­ dá»¥ giá»‘ng nháº¥t
    docs_and_scores = db.similarity_search_with_score(query_text, k=k)

    examples_text = ""
    for doc, score in docs_and_scores:
        # Chá»‰ láº¥y nhá»¯ng vÃ­ dá»¥ thá»±c sá»± liÃªn quan (score tháº¥p lÃ  giá»‘ng, tÃ¹y metric, faiss L2 thÃ¬ tháº¥p lÃ  tá»‘t)
        # á» Ä‘Ã¢y mÃ¬nh cá»© láº¥y háº¿t top k
        ex_ctx = doc.metadata['original_context']
        ex_stmt = doc.metadata['original_statement']
        ex_ans = doc.metadata['answer']

        examples_text += f"---\nVÃ­ dá»¥ tham kháº£o:\nNgá»¯ cáº£nh: {ex_ctx}\nMá»‡nh Ä‘á»: {ex_stmt}\n=> ÄÃ¡p Ã¡n chuáº©n: {ex_ans}\n"

    # 2. GhÃ©p vÃ o Prompt cuá»‘i cÃ¹ng
    final_prompt = f"""DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ vÃ­ dá»¥ tÆ°Æ¡ng tá»± tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ báº¡n tham kháº£o:
{examples_text}
---
Dá»±a vÃ o kiáº¿n thá»©c cá»§a báº¡n vÃ  cÃ¡c vÃ­ dá»¥ trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i sau:
Ngá»¯ cáº£nh: {query_context}
Má»‡nh Ä‘á»: {query_statement}
HÃ£y phÃ¢n loáº¡i má»‡nh Ä‘á» trÃªn lÃ  'ÄÃºng' hoáº·c 'Sai'. Chá»‰ tráº£ lá»i Ä‘Ãºng má»™t tá»«: ÄÃºng hoáº·c Sai.
CÃ¢u tráº£ lá»i:"""
    return final_prompt

# ================= CHáº Y TRÃŠN FILE TEST =================
print(f"ğŸ“‚ Äang Ä‘á»c file Test: {TEST_FILE}")
if TEST_FILE.endswith('.csv'):
    df = pd.read_csv(TEST_FILE)
else:
    df = pd.read_excel(TEST_FILE)

results = []
print("â–¶ï¸ Báº¯t Ä‘áº§u cháº¡y RAG Inference...")

for idx, row in tqdm(df.iterrows(), total=len(df)):
    # Láº¥y dá»¯ liá»‡u test
    ctx = str(row.get('context', '')) if pd.notna(row.get('context')) else ""
    stmt = str(row.get('statement', ''))

    # 1. Táº¡o RAG Prompt (CÃ³ kÃ¨m vÃ­ dá»¥)
    rag_prompt = generate_rag_prompt(ctx, stmt, k=2) # Láº¥y 2 vÃ­ dá»¥ tÆ°Æ¡ng tá»± nháº¥t

    # 2. Tokenize & Generate
    inputs = tokenizer(rag_prompt, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=5,
            temperature=0.01, # Nhiá»‡t Ä‘á»™ tháº¥p Ä‘á»ƒ á»•n Ä‘á»‹nh
            pad_token_id=tokenizer.pad_token_id
        )

    # 3. Decode
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Cáº¯t láº¥y pháº§n tráº£ lá»i cuá»‘i cÃ¹ng
    answer = full_response.split("CÃ¢u tráº£ lá»i:")[-1].strip()

    # Chuáº©n hÃ³a Ä‘áº§u ra
    final_decision = "Sai"
    if "Ä‘Ãºng" in answer.lower(): final_decision = "ÄÃºng"
    elif "sai" in answer.lower(): final_decision = "Sai"

    results.append({
        "Context": ctx,
        "Statement": stmt,
        "Prediction": final_decision,
        "Raw_Response": answer # LÆ°u Ä‘á»ƒ debug xem nÃ³ tráº£ lá»i gÃ¬
    })

# LÆ°u káº¿t quáº£
pd.DataFrame(results).to_excel(OUTPUT_FILE, index=False)
print(f"âœ… Xong! Káº¿t quáº£ lÆ°u táº¡i {OUTPUT_FILE}")

"""# **RAG trÃªn toÃ n bá»™ táº­p dá»¯u liá»‡u bao gá»“m cáº£ file dá»¯ liá»‡u Ä‘Ãºng**"""

import pandas as pd
import json
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
# Import chuáº©n má»›i nháº¥t
from langchain_core.documents import Document

# ================= 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN =================
# CHá»ˆ DÃ™NG FILE Dá»® LIá»†U ÄÃšNG (Dá»¯ liá»‡u chuáº©n)
TRAIN_TRUE_PATH = "/content/drive/MyDrive/KPDLHLV/Data/checkpoint_results.jsonl"

# Model Embedding
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ================= 2. HÃ€M LOAD Dá»® LIá»†U =================
def load_true_data(file_path):
    docs = []

    if not os.path.exists(file_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {file_path}")
        return []

    print(f"ğŸ“‚ Äang Ä‘á»c 'Kho tri thá»©c chuáº©n' tá»«: {file_path}...")
    count = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                line = line.strip()
                if not line: continue

                item = json.loads(line)

                # Chá»‰ láº¥y nhá»¯ng máº«u cÃ³ nhÃ£n lÃ  "ÄÃºng" (hoáº·c True)
                # Tuy nhiÃªn, vÃ¬ file nÃ y báº¡n báº£o lÃ  file toÃ n Ä‘Ãºng, nÃªn ta cá»© láº¥y háº¿t.
                # Náº¿u file láº«n lá»™n thÃ¬ thÃªm if item['answer'] == 'ÄÃºng': ...

                context = item.get('context', '')
                statement = item.get('statement', '')
                answer = "ÄÃºng" # VÃ¬ Ä‘Ã¢y lÃ  file chuáº©n

                # Ná»™i dung Ä‘á»ƒ Embedding
                content_for_search = f"Ngá»¯ cáº£nh: {context}\nMá»‡nh Ä‘á»: {statement}"

                # Metadata
                metadata = {
                    "original_context": context,
                    "original_statement": statement,
                    "answer": answer
                }

                docs.append(Document(page_content=content_for_search, metadata=metadata))
                count += 1
            except Exception as e:
                continue

    print(f"âœ… ÄÃ£ náº¡p {count} kiáº¿n thá»©c chuáº©n vÃ o bá»™ nhá»›.")
    return docs

# ================= 3. THá»°C THI =================

# Load dá»¯ liá»‡u
documents = load_true_data(TRAIN_TRUE_PATH)

if len(documents) > 0:
    print("\nğŸš€ Äang táº¡o Embedding vÃ  Index vÃ o FAISS...")
    print("â³ QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...")

    # Load model embedding
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'}
    )

    # Táº¡o Vector Database
    db = FAISS.from_documents(documents, embedding_model)

    print("âœ… ÄÃ£ xÃ¢y xong RAG Database (Clean Version) thÃ nh cÃ´ng!")
else:
    print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ táº¡o DB.")

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

# ================= Cáº¤U HÃŒNH MODEL FINETUNE =================
ADAPTER_PATH = "/content/drive/MyDrive/KPDLHLV/qwen3_slm_batch6/checkpoint-3800"
BASE_MODEL_ID = "Qwen/Qwen3-0.6B" # (Thá»±c ra lÃ  Qwen2.5-0.5B hoáº·c Qwen2-0.5B, báº¡n check láº¡i Ä‘Ãºng tÃªn nhÃ©)

TEST_FILE = "/content/drive/MyDrive/KPDLHLV/Data/data_test_normalize.csv"
OUTPUT_FILE = "/content/drive/MyDrive/KPDLHLV/Data/Result_RAG_Finetune.xlsx"
DEVICE = "cuda"

# ================= LOAD QWEN MODEL =================
print("ğŸ“¦ Äang load Qwen Finetuned Model...")
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)
if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    device_map=DEVICE,
    torch_dtype=torch.float16,
    trust_remote_code=True
)
base_model.resize_token_embeddings(len(tokenizer))

model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.eval()
print("âœ… Qwen Ä‘Ã£ sáºµn sÃ ng!")

# ================= HÃ€M RAG QUERY =================
def generate_rag_prompt(query_context, query_statement, k=3):
    """
    HÃ m nÃ y tÃ¬m 3 vÃ­ dá»¥ tÆ°Æ¡ng tá»± nháº¥t trong dá»¯ liá»‡u train
    Ä‘á»ƒ lÃ m 'gá»£i Ã½' cho model.
    """
    query_text = f"Ngá»¯ cáº£nh: {query_context}\nMá»‡nh Ä‘á»: {query_statement}"

    # 1. TÃ¬m kiáº¿m trong FAISS
    # k=3: Láº¥y 3 vÃ­ dá»¥ giá»‘ng nháº¥t
    docs_and_scores = db.similarity_search_with_score(query_text, k=k)

    examples_text = ""
    for doc, score in docs_and_scores:
        # Chá»‰ láº¥y nhá»¯ng vÃ­ dá»¥ thá»±c sá»± liÃªn quan (score tháº¥p lÃ  giá»‘ng, tÃ¹y metric, faiss L2 thÃ¬ tháº¥p lÃ  tá»‘t)
        # á» Ä‘Ã¢y mÃ¬nh cá»© láº¥y háº¿t top k
        ex_ctx = doc.metadata['original_context']
        ex_stmt = doc.metadata['original_statement']
        ex_ans = doc.metadata['answer']

        examples_text += f"---\nVÃ­ dá»¥ tham kháº£o:\nNgá»¯ cáº£nh: {ex_ctx}\nMá»‡nh Ä‘á»: {ex_stmt}\n=> ÄÃ¡p Ã¡n chuáº©n: {ex_ans}\n"

    # 2. GhÃ©p vÃ o Prompt cuá»‘i cÃ¹ng
    final_prompt = f"""DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ vÃ­ dá»¥ tÆ°Æ¡ng tá»± tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ báº¡n tham kháº£o:
{examples_text}
---
Dá»±a vÃ o kiáº¿n thá»©c cá»§a báº¡n vÃ  cÃ¡c vÃ­ dá»¥ trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i sau:
Ngá»¯ cáº£nh: {query_context}
Má»‡nh Ä‘á»: {query_statement}
HÃ£y phÃ¢n loáº¡i má»‡nh Ä‘á» trÃªn lÃ  'ÄÃºng' hoáº·c 'Sai'. Chá»‰ tráº£ lá»i Ä‘Ãºng má»™t tá»«: ÄÃºng hoáº·c Sai.
CÃ¢u tráº£ lá»i:"""
    return final_prompt

# ================= CHáº Y TRÃŠN FILE TEST =================
print(f"ğŸ“‚ Äang Ä‘á»c file Test: {TEST_FILE}")
if TEST_FILE.endswith('.csv'):
    df = pd.read_csv(TEST_FILE)
else:
    df = pd.read_excel(TEST_FILE)

results = []
print("â–¶ï¸ Báº¯t Ä‘áº§u cháº¡y RAG Inference...")

for idx, row in tqdm(df.iterrows(), total=len(df)):
    # Láº¥y dá»¯ liá»‡u test
    ctx = str(row.get('context', '')) if pd.notna(row.get('context')) else ""
    stmt = str(row.get('statement', ''))

    # 1. Táº¡o RAG Prompt (CÃ³ kÃ¨m vÃ­ dá»¥)
    rag_prompt = generate_rag_prompt(ctx, stmt, k=2) # Láº¥y 2 vÃ­ dá»¥ tÆ°Æ¡ng tá»± nháº¥t

    # 2. Tokenize & Generate
    inputs = tokenizer(rag_prompt, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=5,
            temperature=0.01, # Nhiá»‡t Ä‘á»™ tháº¥p Ä‘á»ƒ á»•n Ä‘á»‹nh
            pad_token_id=tokenizer.pad_token_id
        )

    # 3. Decode
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Cáº¯t láº¥y pháº§n tráº£ lá»i cuá»‘i cÃ¹ng
    answer = full_response.split("CÃ¢u tráº£ lá»i:")[-1].strip()

    # Chuáº©n hÃ³a Ä‘áº§u ra
    final_decision = "Sai"
    if "Ä‘Ãºng" in answer.lower(): final_decision = "ÄÃºng"
    elif "sai" in answer.lower(): final_decision = "Sai"

    results.append({
        "Context": ctx,
        "Statement": stmt,
        "Prediction": final_decision,
        "Raw_Response": answer # LÆ°u Ä‘á»ƒ debug xem nÃ³ tráº£ lá»i gÃ¬
    })

# LÆ°u káº¿t quáº£
pd.DataFrame(results).to_excel(OUTPUT_FILE, index=False)
print(f"âœ… Xong! Káº¿t quáº£ lÆ°u táº¡i {OUTPUT_FILE}")

"""# **RAG trÃªn toÃ n bá»™ táº­p dá»¯u liá»‡u bao gá»“m cáº£ file dá»¯ liá»‡u Ä‘Ãºng vá»›i mÃ´ hÃ¬nh Qwen gá»‘c**"""

import pandas as pd
import json
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
# Import chuáº©n má»›i nháº¥t
from langchain_core.documents import Document

# ================= 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN =================
# CHá»ˆ DÃ™NG FILE Dá»® LIá»†U ÄÃšNG (Dá»¯ liá»‡u chuáº©n)
TRAIN_TRUE_PATH = "/content/drive/MyDrive/KPDLHLV/Data/checkpoint_results.jsonl"

# Model Embedding
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ================= 2. HÃ€M LOAD Dá»® LIá»†U =================
def load_true_data(file_path):
    docs = []

    if not os.path.exists(file_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {file_path}")
        return []

    print(f"ğŸ“‚ Äang Ä‘á»c 'Kho tri thá»©c chuáº©n' tá»«: {file_path}...")
    count = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                line = line.strip()
                if not line: continue

                item = json.loads(line)

                # Chá»‰ láº¥y nhá»¯ng máº«u cÃ³ nhÃ£n lÃ  "ÄÃºng" (hoáº·c True)
                # Tuy nhiÃªn, vÃ¬ file nÃ y báº¡n báº£o lÃ  file toÃ n Ä‘Ãºng, nÃªn ta cá»© láº¥y háº¿t.
                # Náº¿u file láº«n lá»™n thÃ¬ thÃªm if item['answer'] == 'ÄÃºng': ...

                context = item.get('context', '')
                statement = item.get('statement', '')
                answer = "ÄÃºng" # VÃ¬ Ä‘Ã¢y lÃ  file chuáº©n

                # Ná»™i dung Ä‘á»ƒ Embedding
                content_for_search = f"Ngá»¯ cáº£nh: {context}\nMá»‡nh Ä‘á»: {statement}"

                # Metadata
                metadata = {
                    "original_context": context,
                    "original_statement": statement,
                    "answer": answer
                }

                docs.append(Document(page_content=content_for_search, metadata=metadata))
                count += 1
            except Exception as e:
                continue

    print(f"âœ… ÄÃ£ náº¡p {count} kiáº¿n thá»©c chuáº©n vÃ o bá»™ nhá»›.")
    return docs

# ================= 3. THá»°C THI =================

# Load dá»¯ liá»‡u
documents = load_true_data(TRAIN_TRUE_PATH)

if len(documents) > 0:
    print("\nğŸš€ Äang táº¡o Embedding vÃ  Index vÃ o FAISS...")
    print("â³ QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...")

    # Load model embedding
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'}
    )

    # Táº¡o Vector Database
    db = FAISS.from_documents(documents, embedding_model)

    print("âœ… ÄÃ£ xÃ¢y xong RAG Database (Clean Version) thÃ nh cÃ´ng!")
else:
    print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ táº¡o DB.")

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

# ================= 1. Cáº¤U HÃŒNH (DÃ™NG ÄÃšNG MODEL Báº N YÃŠU Cáº¦U) =================
BASE_MODEL_ID = "Qwen/Qwen3-0.6B" # <-- ÄÃ£ Ä‘á»•i vá» model báº¡n muá»‘n

TEST_FILE = "/content/drive/MyDrive/KPDLHLV/Data/data_test_normalize.csv"
OUTPUT_FILE = "/content/drive/MyDrive/KPDLHLV/Data/Result_RAG_Base_Model_Qwen3.xlsx"
DEVICE = "cuda"

# ================= 2. LOAD MODEL =================
print(f"ğŸ“¦ Äang load Base Model: {BASE_MODEL_ID}...")

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

# Load Model
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    device_map=DEVICE,
    torch_dtype=torch.float16,
    trust_remote_code=True
)
model.eval()
print("âœ… Base Model Ä‘Ã£ sáºµn sÃ ng!")

# ================= 3. HÃ€M RAG QUERY (Tá»I Æ¯U CHO BASE MODEL) =================
def generate_rag_prompt(query_context, query_statement, k=3):
    query_text = f"Ngá»¯ cáº£nh: {query_context}\nMá»‡nh Ä‘á»: {query_statement}"

    # 1. TÃ¬m thÃ´ng tin trong Vector DB (Biáº¿n 'db' tá»« bÆ°á»›c trÆ°á»›c)
    docs_and_scores = db.similarity_search_with_score(query_text, k=k)

    # 2. XÃ¢y dá»±ng ngá»¯ cáº£nh tham kháº£o (Context)
    knowledge_text = ""
    for i, (doc, score) in enumerate(docs_and_scores, 1):
        ex_ctx = doc.metadata['original_context']
        ex_stmt = doc.metadata['original_statement']
        # TrÃ¬nh bÃ y dÆ°á»›i dáº¡ng sá»± tháº­t (Fact)
        knowledge_text += f"Sá»± tháº­t {i}: {ex_ctx} {ex_stmt} (LÃ  thÃ´ng tin ChÃ­nh xÃ¡c).\n"

    # 3. PROMPT Dáº NG HOÃ€N THÃ€NH CÃ‚U (Completion)
    # Base model ráº¥t giá»i viá»‡c Ä‘iá»n tiáº¿p. Ta Ä‘Æ°a cho nÃ³ dá»¯ kiá»‡n vÃ  báº¯t nÃ³ Ä‘iá»n káº¿t luáº­n.
    final_prompt = f"""Dá»±a vÃ o cÃ¡c sá»± tháº­t y khoa sau Ä‘Ã¢y:
{knowledge_text}
---
Dá»±a trÃªn cÃ¡c sá»± tháº­t trÃªn, hÃ£y kiá»ƒm tra má»‡nh Ä‘á» sau:
Ngá»¯ cáº£nh: {query_context}
Má»‡nh Ä‘á»: {query_statement}

CÃ¢u há»i: Má»‡nh Ä‘á» trÃªn lÃ  ÄÃºng hay Sai?
ÄÃ¡p Ã¡n (chá»‰ ghi ÄÃºng hoáº·c Sai):"""
    return final_prompt

# ================= 4. CHáº Y INFERENCE =================
print(f"ğŸ“‚ Äang Ä‘á»c file Test: {TEST_FILE}")
try:
    if TEST_FILE.endswith('.csv'):
        df = pd.read_csv(TEST_FILE)
    else:
        df = pd.read_excel(TEST_FILE)
except Exception as e:
    print(f"âŒ Lá»—i Ä‘á»c file: {e}")
    df = pd.DataFrame()

if not df.empty:
    results = []
    print("â–¶ï¸ Báº¯t Ä‘áº§u cháº¡y Inference...")

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        ctx = str(row.get('context', '')) if pd.notna(row.get('context')) else ""
        stmt = str(row.get('statement', ''))

        try:
            rag_prompt = generate_rag_prompt(ctx, stmt, k=3)
        except NameError:
            print("âŒ Lá»—i: ChÆ°a cÃ³ biáº¿n 'db'. HÃ£y cháº¡y cell táº¡o Vector DB trÆ°á»›c!")
            break

        # Tokenize & Generate
        inputs = tokenizer(rag_prompt, return_tensors="pt").to(DEVICE)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=3, # Chá»‰ cáº§n 2-3 token lÃ  Ä‘á»§ cho chá»¯ "ÄÃºng"/"Sai"
                temperature=0.01, # Nhiá»‡t Ä‘á»™ cá»±c tháº¥p Ä‘á»ƒ model khÃ´ng "sÃ¡ng táº¡o" linh tinh
                pad_token_id=tokenizer.pad_token_id
            )

        # Decode
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Cáº¯t láº¥y pháº§n model vá»«a Ä‘iá»n vÃ o sau dáº¥u hai cháº¥m
        answer = full_response.split("ÄÃ¡p Ã¡n (chá»‰ ghi ÄÃºng hoáº·c Sai):")[-1].strip()

        # Chuáº©n hÃ³a
        final_decision = "Sai"
        if "Ä‘Ãºng" in answer.lower(): final_decision = "ÄÃºng"
        elif "sai" in answer.lower(): final_decision = "Sai"

        results.append({
            "Context": ctx,
            "Statement": stmt,
            "Prediction": final_decision,
            "Raw_Response": answer
        })

    # LÆ°u káº¿t quáº£
    if results:
        pd.DataFrame(results).to_excel(OUTPUT_FILE, index=False)
        print(f"\nâœ… Xong! Káº¿t quáº£ Base Model (Qwen3) lÆ°u táº¡i {OUTPUT_FILE}")

"""# **RAG trÃªn bá»™ dá»¯ liá»‡u Ä‘Ãºng vÃ  VimedAQA vá»›i mÃ´ hÃ¬nh Qwen Ä‘Ã£ finetune**"""

!pip install -q datasets

from datasets import load_dataset
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# 1. Load Dataset tá»« HuggingFace
print("ğŸ“¥ Äang táº£i dataset tmnam20/ViMedAQA...")
dataset = load_dataset("tmnam20/ViMedAQA", split="train")

# 2. Tiá»n xá»­ lÃ½: Chuyá»ƒn Ä‘á»•i QA thÃ nh Document Context
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u...")
knowledge_docs = []

for item in dataset:
    # Láº¥y cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i
    q = item.get('question', '')
    a = item.get('answer', '')

    # Gá»™p láº¡i thÃ nh má»™t Ä‘oáº¡n kiáº¿n thá»©c (Context)
    # Format nÃ y giÃºp embedding model hiá»ƒu ngá»¯ nghÄ©a trá»n váº¹n
    content = f"CÃ¢u há»i y táº¿: {q}\nThÃ´ng tin giáº£i Ä‘Ã¡p: {a}"

    # Metadata lÆ°u nguá»“n
    metadata = {"source": "ViMedAQA", "type": "knowledge"}

    knowledge_docs.append(Document(page_content=content, metadata=metadata))

print(f"âœ… ÄÃ£ xá»­ lÃ½ {len(knowledge_docs)} máº«u kiáº¿n thá»©c tá»« ViMedAQA.")

# 3. Táº¡o Vector DB cho Kiáº¿n thá»©c (Knowledge Index)
# LÆ°u Ã½: DÃ¹ng chung model embedding vá»›i pháº§n trÆ°á»›c Ä‘á»ƒ Ä‘á»“ng bá»™
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

print("ğŸš€ Äang táº¡o Knowledge Index (FAISS)...")
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={'device': 'cuda'}
)

knowledge_db = FAISS.from_documents(knowledge_docs, embedding_model)
print("âœ… ÄÃ£ táº¡o xong Knowledge Database (tá»« ViMedAQA)!")

import json
import os
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS

# 1. Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n (Chá»‰ file ÄÃšNG)
TRAIN_TRUE_PATH = "/content/drive/MyDrive/KPDLHLV/Data/checkpoint_results.jsonl"

def load_true_only_data(file_path):
    docs = []

    if not os.path.exists(file_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {file_path}")
        return []

    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u vÃ­ dá»¥ tá»«: {file_path}...")
    count = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                line = line.strip()
                if not line: continue
                item = json.loads(line)

                context = item.get('context', '')
                statement = item.get('statement', '')
                # Máº·c Ä‘á»‹nh lÃ  ÄÃºng vÃ¬ báº¡n chá»‰ load file Ä‘Ãºng
                answer = item.get('answer', 'ÄÃºng')

                # Ná»™i dung Ä‘á»ƒ tÃ¬m kiáº¿m (Statement lÃ  chÃ­nh)
                content_for_search = f"Má»‡nh Ä‘á»: {statement}"

                # Metadata Ä‘áº§y Ä‘á»§
                metadata = {
                    "original_context": context,
                    "original_statement": statement,
                    "answer": answer,
                    "type": "example"
                }

                docs.append(Document(page_content=content_for_search, metadata=metadata))
                count += 1
            except:
                continue

    print(f"âœ… ÄÃ£ náº¡p {count} vÃ­ dá»¥ máº«u (nhÃ£n ÄÃºng).")
    return docs

# 2. Táº¡o Vector DB (biáº¿n 'db')
# LÆ°u Ã½: Biáº¿n 'embedding_model' Ä‘Ã£ Ä‘Æ°á»£c táº¡o á»Ÿ cell ViMedAQA trÆ°á»›c Ä‘Ã³
if 'embedding_model' not in globals():
    print("âš ï¸ Cáº£nh bÃ¡o: ChÆ°a cÃ³ biáº¿n embedding_model. HÃ£y cháº¡y cell ViMedAQA trÆ°á»›c!")
else:
    print("ğŸš€ Äang táº¡o Index cho vÃ­ dá»¥ máº«u...")
    example_docs = load_true_only_data(TRAIN_TRUE_PATH)

    if len(example_docs) > 0:
        db = FAISS.from_documents(example_docs, embedding_model)
        print("âœ… ÄÃ£ táº¡o xong kho vÃ­ dá»¥ máº«u 'db' (Chá»‰ chá»©a dá»¯ liá»‡u ÄÃºng)!")
    else:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ táº¡o DB.")

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import gc

# ================= 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN =================
# ÄÆ°á»ng dáº«n Ä‘áº¿n folder checkpoint adapter Ä‘Ã£ fine-tune
ADAPTER_PATH = "/content/drive/MyDrive/KPDLHLV/qwen3_slm_batch6/checkpoint-3800"

# TÃªn Base Model gá»‘c (Sá»­a láº¡i Ä‘Ãºng theo yÃªu cáº§u Qwen3-0.6B cá»§a báº¡n)
# LÆ°u Ã½: HÃ£y cháº¯c cháº¯n tÃªn nÃ y Ä‘Ãºng vá»›i tÃªn model báº¡n dÃ¹ng trong file train
BASE_MODEL_ID = "Qwen/Qwen3-0.6B"

# ================= 2. Dá»ŒN Dáº¸P Bá»˜ NHá»š =================
try:
    del model
    del base_model
    del tokenizer
except:
    pass
torch.cuda.empty_cache()
gc.collect()
print("ğŸ§¹ ÄÃ£ dá»n dáº¹p bá»™ nhá»› GPU.")

# ================= 3. LOAD MODEL & ADAPTER =================
print(f"ğŸ“¦ Äang táº£i Base Model: {BASE_MODEL_ID}...")
device = "cuda" if torch.cuda.is_available() else "cpu"

try:
    # Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load Base Model
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )

    # Resize token embedding
    base_model.resize_token_embeddings(len(tokenizer))

    print(f"ğŸ”— Äang ghÃ©p Adapter tá»«: {ADAPTER_PATH}...")
    # Load Adapter
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval()

    print(f"âœ… ÄÃ£ náº¡p thÃ nh cÃ´ng Model {BASE_MODEL_ID} Fine-tune!")

    # ================= 4. TEST NHANH =================
    test_input = "Context: Bá»‡nh cÃºm lÃ  bá»‡nh truyá»n nhiá»…m.\nStatement: Bá»‡nh cÃºm khÃ´ng lÃ¢y.\nAnswer:"
    inputs = tokenizer(test_input, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=10)
        print("\n--- Test Output ---")
        print(tokenizer.decode(outputs[0], skip_special_tokens=True))

except OSError as e:
    print(f"\nâŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y model '{BASE_MODEL_ID}' trÃªn Hugging Face hoáº·c Ä‘Æ°á»ng dáº«n sai.")
    print("ğŸ‘‰ Báº¡n hÃ£y kiá»ƒm tra láº¡i chÃ­nh xÃ¡c tÃªn Base Model Ä‘Ã£ dÃ¹ng lÃºc train vÃ  sá»­a láº¡i biáº¿n BASE_MODEL_ID.")
    print(f"Chi tiáº¿t lá»—i: {e}")

import torch
import pandas as pd
from tqdm import tqdm

# Äáº£m báº£o báº¡n Ä‘Ã£ load model vÃ  tokenizer á»Ÿ cÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã³
# VÃ  Ä‘Ã£ cÃ³ biáº¿n 'example_db' (lÃ  biáº¿n 'db' cÅ© chá»©a dá»¯ liá»‡u train ÄÃºng/Sai)

def generate_enhanced_prompt(user_statement, k_knowledge=1, k_example=2):
    """
    HÃ m táº¡o prompt káº¿t há»£p:
    1. TÃ¬m kiáº¿n thá»©c tá»« ViMedAQA Ä‘á»ƒ lÃ m Context.
    2. TÃ¬m vÃ­ dá»¥ máº«u tá»« dá»¯ liá»‡u Train Ä‘á»ƒ lÃ m Few-shot.
    """

    # BÆ¯á»šC 1: TÃ¬m Context tá»« ViMedAQA
    # Truy váº¥n báº±ng chÃ­nh statement cá»§a ngÆ°á»i dÃ¹ng
    knowledge_results = knowledge_db.similarity_search(user_statement, k=k_knowledge)

    # Láº¥y ná»™i dung tá»‘t nháº¥t lÃ m Context
    retrieved_context = ""
    if knowledge_results:
        retrieved_context = knowledge_results[0].page_content
    else:
        retrieved_context = "KhÃ´ng cÃ³ thÃ´ng tin bá»• sung."

    # BÆ¯á»šC 2: TÃ¬m Few-shot Examples tá»« Dá»¯ liá»‡u Train cÅ© (biáº¿n db cÅ©)
    # TÃ¬m vÃ­ dá»¥ cÃ³ cáº¥u trÃºc tÆ°Æ¡ng tá»±
    # LÆ°u Ã½: 'db' lÃ  biáº¿n vector store chá»©a dá»¯ liá»‡u train Ä‘Ãºng/sai báº¡n Ä‘Ã£ táº¡o tá»« trÆ°á»›c
    query_for_example = f"Má»‡nh Ä‘á»: {user_statement}"
    example_results = db.similarity_search(query_for_example, k=k_example)

    examples_text = ""
    for doc in example_results:
        ex_ctx = doc.metadata.get('original_context', '')
        ex_stmt = doc.metadata.get('original_statement', '')
        ex_ans = doc.metadata.get('answer', '')
        # Format vÃ­ dá»¥ ngáº¯n gá»n
        examples_text += f"---\nVÃ­ dá»¥:\nNgá»¯ cáº£nh: {ex_ctx}\nMá»‡nh Ä‘á»: {ex_stmt}\n=> ÄÃ¡p Ã¡n: {ex_ans}\n"

    # BÆ¯á»šC 3: GhÃ©p Prompt
    # ÄÃ¢y lÃ  Prompt tá»‘i Æ°u cho model Ä‘Ã£ fine-tune
    final_prompt = f"""DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ vÃ­ dá»¥ máº«u vá» cÃ¡ch kiá»ƒm chá»©ng má»‡nh Ä‘á» y khoa:
{examples_text}
---
Dá»±a vÃ o kiáº¿n thá»©c y khoa dÆ°á»›i Ä‘Ã¢y (Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u ViMedAQA):
{retrieved_context}

HÃ£y kiá»ƒm tra má»‡nh Ä‘á» sau:
Má»‡nh Ä‘á»: {user_statement}

HÃ£y phÃ¢n loáº¡i má»‡nh Ä‘á» trÃªn lÃ  'ÄÃºng' hoáº·c 'Sai'.
CÃ¢u tráº£ lá»i:"""

    return final_prompt, retrieved_context

# --- CHáº Y THá»¬ NGHIá»†M ---
TEST_FILE = "/content/drive/MyDrive/KPDLHLV/Data/data_test_normalize.csv" # Äá»•i Ä‘Æ°á»ng dáº«n náº¿u cáº§n
OUTPUT_FILE = "/content/drive/MyDrive/KPDLHLV/Data/Result_Hybrid_RAG.xlsx"

print("â–¶ï¸ Báº¯t Ä‘áº§u cháº¡y Hybrid RAG (ViMedAQA Context + Few-shot)...")

if TEST_FILE.endswith('.csv'):
    df_test = pd.read_csv(TEST_FILE)
else:
    df_test = pd.read_excel(TEST_FILE)

results = []

# Äá»ƒ test nhanh, mÃ¬nh cháº¡y thá»­ 100 cÃ¢u Ä‘áº§u (bá» [:100] Ä‘á»ƒ cháº¡y háº¿t)
for idx, row in tqdm(df_test.iterrows(), total=len(df_test)):
    stmt = str(row.get('statement', ''))

    # Táº¡o Prompt káº¿t há»£p
    # LÆ°u Ã½: Náº¿u biáº¿n vector store cÅ© cá»§a báº¡n tÃªn lÃ  'db', hÃ m trÃªn sáº½ tá»± dÃ¹ng nÃ³.
    rag_prompt, used_context = generate_enhanced_prompt(stmt, k_knowledge=1, k_example=2)

    inputs = tokenizer(rag_prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=5,
            temperature=0.01,
            pad_token_id=tokenizer.pad_token_id
        )

    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer_part = full_response.split("CÃ¢u tráº£ lá»i:")[-1].strip()

    # Logic láº¥y nhÃ£n
    final_decision = "Sai" # Máº·c Ä‘á»‹nh an toÃ n
    if "Ä‘Ãºng" in answer_part.lower(): final_decision = "ÄÃºng"
    elif "sai" in answer_part.lower(): final_decision = "Sai"

    results.append({
        "Statement": stmt,
        "Retrieved_Knowledge_ViMedAQA": used_context, # LÆ°u láº¡i xem nÃ³ láº¥y Ä‘Æ°á»£c kiáº¿n thá»©c gÃ¬
        "Model_Prediction": final_decision,
        "Raw_Output": answer_part
    })

# LÆ°u káº¿t quáº£
pd.DataFrame(results).to_excel(OUTPUT_FILE, index=False)
print(f"âœ… HoÃ n táº¥t! Káº¿t quáº£ lÆ°u táº¡i: {OUTPUT_FILE}")