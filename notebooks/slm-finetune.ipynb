{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-13T06:34:47.324241Z",
     "iopub.status.busy": "2025-12-13T06:34:47.324001Z",
     "iopub.status.idle": "2025-12-13T06:36:34.252086Z",
     "shell.execute_reply": "2025-12-13T06:36:34.251153Z",
     "shell.execute_reply.started": "2025-12-13T06:34:47.324221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q protobuf==3.20.3\n",
    "!pip install -q transformers --upgrade\n",
    "!pip install -q accelerate --upgrade\n",
    "!pip install -q peft --upgrade\n",
    "!pip install -q bitsandbytes --upgrade\n",
    "!pip install -q datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T06:37:44.039062Z",
     "iopub.status.busy": "2025-12-13T06:37:44.038771Z",
     "iopub.status.idle": "2025-12-13T06:37:44.470824Z",
     "shell.execute_reply": "2025-12-13T06:37:44.469805Z",
     "shell.execute_reply.started": "2025-12-13T06:37:44.039040Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Äang load dá»¯ liá»‡u...\n",
      "âœ… ÄÃ£ load 20000 máº«u tá»« /kaggle/input/data-finetune/checkpoint_results.jsonl\n",
      "âœ… ÄÃ£ load 20000 máº«u tá»« /kaggle/input/data-finetune/checkpoint_results_sai.jsonl\n",
      "\n",
      "âœ… Tá»•ng sá»‘ máº«u: 40000\n",
      "   - Train: 38000\n",
      "   - Eval: 2000\n",
      "\n",
      "ğŸ“¦ Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho training!\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 1: Load vÃ  xá»­ lÃ½ dá»¯ liá»‡u =====\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def load_jsonl(path: str, fallback_label: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"Load JSONL file tá»« Google Drive\"\"\"\n",
    "    data = []\n",
    "    if not path or not os.path.exists(path):\n",
    "        print(f\"âš ï¸ File khÃ´ng tá»“n táº¡i: {path}\")\n",
    "        return data\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            answer = obj.get(\"answer\") or fallback_label\n",
    "            if answer is None:\n",
    "                continue\n",
    "            obj[\"answer\"] = answer\n",
    "            data.append(obj)\n",
    "    \n",
    "    print(f\"âœ… ÄÃ£ load {len(data)} máº«u tá»« {path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_answer(ans: str) -> str:\n",
    "    \"\"\"Chuáº©n hÃ³a cÃ¢u tráº£ lá»i vá» ÄÃºng/Sai\"\"\"\n",
    "    ans = ans.strip().lower()\n",
    "    if ans.startswith(\"Ä‘\"):\n",
    "        return \"ÄÃºng\"\n",
    "    if ans.startswith(\"s\"):\n",
    "        return \"Sai\"\n",
    "    return \"Sai\"\n",
    "\n",
    "\n",
    "def build_messages(context: str, statement: str, answer: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Táº¡o format chat messages cho Qwen\"\"\"\n",
    "    display_context = context.strip() or statement.strip()\n",
    "    display_statement = statement.strip() or context.strip()\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Ngá»¯ cáº£nh: {context}\\n\"\n",
    "        \"Má»‡nh Ä‘á»: {statement}\\n\"\n",
    "        \"HÃ£y phÃ¢n loáº¡i má»‡nh Ä‘á» trÃªn lÃ  'ÄÃºng' hoáº·c 'Sai'. \"\n",
    "        \"Chá»‰ tráº£ lá»i Ä‘Ãºng má»™t tá»«: ÄÃºng hoáº·c Sai.\"\n",
    "    ).format(context=display_context, statement=display_statement)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Báº¡n lÃ  chuyÃªn gia y táº¿, chá»‰ tráº£ lá»i 'ÄÃºng' hoáº·c 'Sai'.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "\n",
    "\n",
    "# ===== Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN =====\n",
    "TRAIN_FILE = \"/kaggle/input/data-finetune/checkpoint_results.jsonl\"\n",
    "EXTRA_FILE = \"/kaggle/input/data-finetune/checkpoint_results_sai.jsonl\"\n",
    "\n",
    "# ===== LOAD Dá»® LIá»†U =====\n",
    "print(\"ğŸ“Š Äang load dá»¯ liá»‡u...\")\n",
    "train_data = load_jsonl(TRAIN_FILE)\n",
    "extra = load_jsonl(EXTRA_FILE) if EXTRA_FILE else []\n",
    "all_data = train_data + extra\n",
    "\n",
    "# Shuffle dá»¯ liá»‡u\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "if len(all_data) == 0:\n",
    "    raise ValueError(\"âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n. Kiá»ƒm tra Ä‘Æ°á»ng dáº«n file!\")\n",
    "\n",
    "# Split train/eval (95%/5%)\n",
    "split_idx = int(0.95 * len(all_data))\n",
    "train_examples = all_data[:split_idx]\n",
    "eval_examples = all_data[split_idx:]\n",
    "\n",
    "print(f\"\\nâœ… Tá»•ng sá»‘ máº«u: {len(all_data)}\")\n",
    "print(f\"   - Train: {len(train_examples)}\")\n",
    "print(f\"   - Eval: {len(eval_examples)}\")\n",
    "print(f\"\\nğŸ“¦ Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T06:56:19.422511Z",
     "iopub.status.busy": "2025-12-13T06:56:19.421790Z",
     "iopub.status.idle": "2025-12-13T16:55:33.158065Z",
     "shell.execute_reply": "2025-12-13T16:55:33.157119Z",
     "shell.execute_reply.started": "2025-12-13T06:56:19.422460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Äang táº£i model Qwen/Qwen3-0.6B...\n",
      "ğŸ“¦ Äang chuáº©n bá»‹ dataset...\n",
      "\n",
      "ğŸï¸ Báº¯t Ä‘áº§u training (Batch 6)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3801' max='3801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3801/3801 9:57:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.003404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.010995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.004094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.003416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.003132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.003606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.002474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.002104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HoÃ n táº¥t!\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL: TRAINING (CUSTOM MODE - BATCH 6) =====\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# --- 1. Dá»ŒN Dáº¸P & Cáº¤U HÃŒNH ---\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- 2. HÃ€M Xá»¬ LÃ (Giá»¯ nguyÃªn) ---\n",
    "def normalize_answer(ans):\n",
    "    if not isinstance(ans, str): return \"Sai\"\n",
    "    ans = ans.strip().lower()\n",
    "    if ans.startswith(\"Ä‘\"): return \"ÄÃºng\"\n",
    "    if ans.startswith(\"s\"): return \"Sai\"\n",
    "    return \"Sai\"\n",
    "\n",
    "def build_messages(context, statement, answer):\n",
    "    display_context = context.strip() if context else (statement.strip() if statement else \"\")\n",
    "    display_statement = statement.strip() if statement else display_context\n",
    "    user_prompt = (\n",
    "        f\"Ngá»¯ cáº£nh: {display_context}\\n\"\n",
    "        f\"Má»‡nh Ä‘á»: {display_statement}\\n\"\n",
    "        \"HÃ£y phÃ¢n loáº¡i má»‡nh Ä‘á» trÃªn lÃ  'ÄÃºng' hoáº·c 'Sai'. \"\n",
    "        \"Chá»‰ tráº£ lá»i Ä‘Ãºng má»™t tá»«: ÄÃºng hoáº·c Sai.\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"Báº¡n lÃ  chuyÃªn gia y táº¿, chá»‰ tráº£ lá»i 'ÄÃºng' hoáº·c 'Sai'.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "\n",
    "# --- 3. DATASET & MODEL ---\n",
    "class QwenLazyDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_len):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        single_text = example.get(\"text\") or example.get(\"sentence\") or example.get(\"content\") or \"\"\n",
    "        context = example.get(\"context\", \"\") or single_text\n",
    "        statement = example.get(\"statement\", \"\") or single_text\n",
    "        answer = normalize_answer(example.get(\"answer\", \"Sai\"))\n",
    "        \n",
    "        messages = build_messages(context, statement, answer)\n",
    "        prompt_text = self.tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n",
    "        full_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        \n",
    "        prompt_ids = self.tokenizer(prompt_text, truncation=True, max_length=self.max_len)[\"input_ids\"]\n",
    "        enc = self.tokenizer(full_text, truncation=True, max_length=self.max_len, padding=False, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids = enc[\"input_ids\"][0]\n",
    "        labels = input_ids.clone()\n",
    "        labels[:len(prompt_ids)] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": enc[\"attention_mask\"][0], \"labels\": labels}\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/Checkpoint/qwen3_slm_batch6\"\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "print(f\"ğŸš€ Äang táº£i model {MODEL_ID}...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, quantization_config=bnb_config, torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --- Báº®T BUá»˜C: Checkpointing ---\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- 4. TRAINER CONFIG ---\n",
    "print(\"ğŸ“¦ Äang chuáº©n bá»‹ dataset...\")\n",
    "train_dataset = QwenLazyDataset(train_examples, tokenizer, max_len=512)\n",
    "eval_dataset = QwenLazyDataset(eval_examples, tokenizer, max_len=512)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # --- Cáº¤U HÃŒNH BATCH 6 ---\n",
    "    per_device_train_batch_size=6,    # TÄƒng lÃªn 6\n",
    "    gradient_accumulation_steps=5,    # 6 * 5 = 30 (Gáº§n tÆ°Æ¡ng Ä‘Æ°Æ¡ng batch size 32 chuáº©n)\n",
    "    gradient_checkpointing=True,      # GIá»® NGUYÃŠN Ä‘á»ƒ khÃ´ng bá»‹ lá»—i bá»™ nhá»›\n",
    "    dataloader_num_workers=2,         \n",
    "    # ------------------------\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\nğŸï¸ Báº¯t Ä‘áº§u training (Batch 6)...\")\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "model.config.use_cache = True\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"âœ… HoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:00:38.495106Z",
     "iopub.status.busy": "2025-12-13T17:00:38.494794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Äang load model Ä‘á»ƒ kiá»ƒm tra...\n",
      "âœ… Model Ä‘Ã£ sáºµn sÃ ng!\n",
      "ğŸ“„ ÄÃ£ load file CSV: 1246 dÃ²ng\n",
      "â³ Äang cháº¡y dá»± Ä‘oÃ¡n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 664/1246 [04:33<03:56,  2.47it/s]"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Cáº¤U HÃŒNH & LOAD MODEL (ÄÃƒ Sá»¬A Lá»–I)\n",
    "# ==========================================\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ÄÆ°á»ng dáº«n (Giá»¯ nguyÃªn cá»§a báº¡n)\n",
    "ADAPTER_PATH = \"/kaggle/working/Checkpoint/qwen3_slm_batch6\" \n",
    "BASE_MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "TEST_FILE_PATH = \"/kaggle/input/data-test/data_test_normalize.csv\" \n",
    "\n",
    "print(\"ğŸš€ Äang load model Ä‘á»ƒ kiá»ƒm tra...\")\n",
    "\n",
    "# --- Sá»¬A Äá»”I 1: Load Tokenizer tá»« ADAPTER_PATH (thay vÃ¬ BASE_MODEL_ID) ---\n",
    "# LÃ½ do: Äá»ƒ láº¥y Ä‘Ãºng cÃ¡i tokenizer Ä‘Ã£ dÃ¹ng lÃºc train\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)\n",
    "except:\n",
    "    # Fallback náº¿u khÃ´ng tÃ¬m tháº¥y trong adapter path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model gá»‘c\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16, # Hoáº·c float16 tÃ¹y GPU\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# --- Sá»¬A Äá»”I 2: RESIZE Láº I MODEL Gá»C (QUAN TRá»ŒNG NHáº¤T) ---\n",
    "# Ã‰p kÃ­ch thÆ°á»›c model gá»‘c pháº£i báº±ng Ä‘Ãºng kÃ­ch thÆ°á»›c lÃºc train (151669)\n",
    "# Náº¿u tokenizer load Ä‘Ãºng, len(tokenizer) sáº½ lÃ  151669\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Gáº¯n Adapter (LoRA) vÃ o\n",
    "# BÃ¢y giá» kÃ­ch thÆ°á»›c Ä‘Ã£ khá»›p, lá»‡nh nÃ y sáº½ cháº¡y ngon lÃ nh\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model Ä‘Ã£ sáºµn sÃ ng!\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. HÃ€M CHUáº¨N HÃ“A & Dá»° ÄOÃN\n",
    "# ==========================================\n",
    "def normalize_ground_truth(value):\n",
    "    \"\"\"\n",
    "    Chuáº©n hÃ³a cá»™t True/False trong file CSV vá» 'ÄÃºng'/'Sai'\n",
    "    Ä‘á»ƒ so sÃ¡nh vá»›i model.\n",
    "    \"\"\"\n",
    "    s = str(value).strip().lower()\n",
    "    if s in ['true', '1', 't', 'yes', 'Ä‘Ãºng']:\n",
    "        return 'ÄÃºng'\n",
    "    if s in ['false', '0', 'f', 'no', 'sai']:\n",
    "        return 'Sai'\n",
    "    return 'KhÃ´ng xÃ¡c Ä‘á»‹nh'\n",
    "\n",
    "def predict_row(row):\n",
    "    \"\"\"HÃ m dá»± Ä‘oÃ¡n cho tá»«ng dÃ²ng\"\"\"\n",
    "    # Láº¥y dá»¯ liá»‡u tá»« cÃ¡c cá»™t (Sá»­a tÃªn cá»™t náº¿u file CSV cá»§a báº¡n khÃ¡c)\n",
    "    context = str(row.get('context', '')) \n",
    "    statement = str(row.get('statement', ''))\n",
    "    \n",
    "    # Format giá»‘ng há»‡t lÃºc train\n",
    "    display_context = context if context else statement\n",
    "    display_statement = statement if statement else display_context\n",
    "    \n",
    "    prompt = f\"\"\"Ngá»¯ cáº£nh: {display_context}\n",
    "Má»‡nh Ä‘á»: {display_statement}\n",
    "HÃ£y phÃ¢n loáº¡i má»‡nh Ä‘á» trÃªn lÃ  'ÄÃºng' hoáº·c 'Sai'. Chá»‰ tráº£ lá»i Ä‘Ãºng má»™t tá»«: ÄÃºng hoáº·c Sai.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Báº¡n lÃ  chuyÃªn gia y táº¿, chá»‰ tráº£ lá»i 'ÄÃºng' hoáº·c 'Sai'.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Tokenize vÃ  cháº¡y model\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=5, # Chá»‰ cáº§n ngáº¯n thÃ´i\n",
    "            temperature=0.01, # Gáº§n 0 Ä‘á»ƒ káº¿t quáº£ nháº¥t quÃ¡n\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "    # Giáº£i mÃ£ káº¿t quáº£\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Láº¥y pháº§n model tráº£ lá»i (sau chá»¯ assistant)\n",
    "    answer = result.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº§u ra cá»§a model (Ä‘Ã´i khi nÃ³ tráº£ lá»i thÃªm dáº¥u cháº¥m)\n",
    "    if \"Ä‘Ãºng\" in answer.lower(): return \"ÄÃºng\"\n",
    "    if \"sai\" in answer.lower(): return \"Sai\"\n",
    "    return \"Sai\" # Máº·c Ä‘á»‹nh náº¿u model nÃ³i linh tinh\n",
    "\n",
    "# ==========================================\n",
    "# 3. CHáº Y TRÃŠN FILE CSV\n",
    "# ==========================================\n",
    "# Load file CSV\n",
    "try:\n",
    "    df = pd.read_csv(TEST_FILE_PATH)\n",
    "    print(f\"ğŸ“„ ÄÃ£ load file CSV: {len(df)} dÃ²ng\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Lá»—i load file CSV: {e}\")\n",
    "    # Táº¡o data giáº£ Ä‘á»ƒ test code náº¿u chÆ°a cÃ³ file\n",
    "    df = pd.DataFrame([\n",
    "        {\"context\": \"Bá»‡nh nhÃ¢n sá»‘t cao 39 Ä‘á»™.\", \"statement\": \"Bá»‡nh nhÃ¢n bá»‹ háº¡ thÃ¢n nhiá»‡t.\", \"label\": False},\n",
    "        {\"context\": \"ViÃªm gan B lÃ¢y qua Ä‘Æ°á»ng mÃ¡u.\", \"statement\": \"ViÃªm gan B lÃ  bá»‡nh truyá»n nhiá»…m.\", \"label\": True}\n",
    "    ])\n",
    "\n",
    "# âš ï¸ QUAN TRá»ŒNG: Chuáº©n hÃ³a cá»™t Label (Káº¿t quáº£ Ä‘Ãºng) trong file CSV\n",
    "# Báº¡n hÃ£y thay 'label' báº±ng tÃªn cá»™t chá»©a True/False trong file cá»§a báº¡n (vÃ­ dá»¥: 'answer', 'Ground Truth'...)\n",
    "COL_LABEL_NAME = 'answer' # <--- Sá»¬A TÃŠN Cá»˜T LABEL Cá»¦A Báº N á» ÄÃ‚Y\n",
    "\n",
    "if COL_LABEL_NAME in df.columns:\n",
    "    df['Chuan_Hoa'] = df[COL_LABEL_NAME].apply(normalize_ground_truth)\n",
    "else:\n",
    "    print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y cá»™t '{COL_LABEL_NAME}'. Code sáº½ chá»‰ cháº¡y dá»± Ä‘oÃ¡n.\")\n",
    "\n",
    "# Cháº¡y dá»± Ä‘oÃ¡n (DÃ¹ng tqdm Ä‘á»ƒ hiá»‡n thanh loading)\n",
    "tqdm.pandas()\n",
    "print(\"â³ Äang cháº¡y dá»± Ä‘oÃ¡n...\")\n",
    "df['Model_Du_Doan'] = df.progress_apply(predict_row, axis=1)\n",
    "\n",
    "# TÃ­nh Ä‘á»™ chÃ­nh xÃ¡c (náº¿u cÃ³ cá»™t label)\n",
    "if COL_LABEL_NAME in df.columns:\n",
    "    df['Ket_Qua'] = df['Model_Du_Doan'] == df['Chuan_Hoa']\n",
    "    acc = df['Ket_Qua'].mean()\n",
    "    print(f\"\\nğŸ¯ Äá»˜ CHÃNH XÃC: {acc:.2%}\")\n",
    "\n",
    "# LÆ°u káº¿t quáº£ ra file má»›i\n",
    "OUTPUT_CSV = \"ket_qua_kiem_tra.csv\"\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ táº¡i: {OUTPUT_CSV}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u\n",
    "print(df[['Model_Du_Doan', 'Chuan_Hoa']].head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9008804,
     "sourceId": 14137177,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9013548,
     "sourceId": 14143506,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
